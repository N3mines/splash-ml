{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitenvvenv75e8014f2ae841849f8ca7dd97989cd7",
   "display_name": "Python 3.8.2 64-bit ('env': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Sample ingest, tag and query data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This notebook demonstrates how to use Splash-ML to ingest data into databroker using the ETL package, how to save tag sets using the TagService in the tagging packages, and how to query on those tags.\n",
    "\n",
    "The notebook uses mongomock to mimic a mongo database instance in memory."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '../..')\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import tempfile\n",
    "from mongomock import MongoClient\n",
    "from suitcase.mongo_normalized import Serializer\n",
    "\n",
    "import etl.ingest\n",
    "from tagging.tag_service import TagService"
   ]
  },
  {
   "source": [
    "First, let's do some setup. We'll create a monomock instance which will be used by as a location to ingest data into and as a place to save and search on tags."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The `ETLExecutor` does a variety of things. It's provided an `input_root`, which is a directory that the `ETLExecutor` will scan recursively, finding assets to trasform and ingest into databroker.  The transformed original files and transformed files are stored in an anonymized location, specified by the `output_root` parameter. Finally, a bluesky document stream is produced.\n",
    "\n",
    "\n",
    "An optional parameter can be provided: `properties_callback`. The takes a callable that can be used to extract properties data from each file as it is ingested. One usecase for this is that many existing datasets are saved in large file systems with much metadata implied by the file path and file names. This callback can be used to extract that metdata as the asset is ingested.\n",
    "\n",
    "Information from each file's transformations can then be used in the ETLExecutor.createDocument, which is a databroker 'ingestor', creating a document set for the asset and its \"thumbnails\"\n",
    "\n",
    "Let's create a properties_callback. This callable is used by the `ETLExecutor` to build up a list of detected properties as it "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/tmp/tmpa5d99y7l\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['source/radscan.tiff', 'source/agb.tiff']"
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "source": [
    "def properties_callback(path):\n",
    "    metadata = {}\n",
    "    if 'agb' in path:\n",
    "        metadata['scan_type'] = 'agb_calibration'\n",
    "    return metadata\n",
    "\n",
    "tagging_event_uid = ''\n",
    "\n",
    "# create a tagger and tagging event reference for tags to reference\n",
    "def setup_tagging():\n",
    "    now = datetime.datetime.utcnow().replace(tzinfo = datetime.timezone.utc) \\\n",
    "        .astimezone().replace(microsecond = 0).isoformat()\n",
    "    tagger_uid = tag_svc.create_tagger(\n",
    "        {\n",
    "            'uid': None,\n",
    "            'type': 'model',\n",
    "            'model_name': 'sample notebook',\n",
    "            'create_time': now\n",
    "        })\n",
    "\n",
    "    tagging_event_uid = tag_svc.create_tagging_event(\n",
    "        {\n",
    "            'uid': None,\n",
    "            'tagger_id': None,\n",
    "            'run_time': now,\n",
    "            'accuracy': 0.9234\n",
    "        },\n",
    "        tagger_uid)\n",
    "\n",
    "\n",
    "def make_tag_set(start_doc):\n",
    "    tags = []\n",
    "    if start_doc.get('scan_type') == 'agb_calibration':\n",
    "        tags.append({\n",
    "            'tag': 'agb',\n",
    "            'confidence': 0.99\n",
    "        })\n",
    "    else:\n",
    "        tags.append({\n",
    "            'tag': 'sample',\n",
    "            'confidence': 0.99\n",
    "        })  \n",
    "    \n",
    "    return {\n",
    "        'sample_id': 'random',\n",
    "        'tags': tags\n",
    "    }\n",
    "\n",
    "\n",
    "db = MongoClient()\n",
    "input_dir = 'source'\n",
    "output_dir = tempfile.TemporaryDirectory().name\n",
    "print(output_dir)\n",
    "#use glob to find all the files to ingest\n",
    "paths = glob.glob(os.path.join(input_dir + '/**/*.*'), recursive=True)\n",
    "etl_executor = ETLExecutor(input_dir, output_dir, properties_callback)\n",
    "serializer = Serializer(\n",
    "        db['databroker_db_name'],\n",
    "        db['databroker_db_name'])\n",
    "\n",
    "tag_svc = TagService(db, db_name='tagging')\n",
    "setup_tagging()\n",
    "paths"
   ]
  },
  {
   "source": [
    "OK, now that we've set everything up, let's go ahead and execute. We're pointed it the included source directory, that contains two sample tiff files.\n",
    "\n",
    "We're going to execute, instructing the `etl_executor` to convert files to jpg and npy."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING:fabio.tifimage:Unable to read /tmp/tmpa5d99y7l/d41d8cd98f00b204e9800998ecf8427e/4f1573eb-41dc-44e7-9de5-8926b3ad639c.tiff with TiffIO due to %d format: a number is required, not bytes, trying PIL\nWARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\nWARNING:fabio.tifimage:Unable to read /tmp/tmpa5d99y7l/d41d8cd98f00b204e9800998ecf8427e/b4d73672-acbf-4d3a-bce3-6b4aedc82664.tiff with TiffIO due to %d format: a number is required, not bytes, trying PIL\nWARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\nsource/radscan.tiff\n\nreading and transforming file: source/radscan.tiff\nCreating tags for: {}\n{'uid': '4f1573eb-41dc-44e7-9de5-8926b3ad639c', 'time': 1600040902.95923}\nsource/agb.tiff\n\nreading and transforming file: source/agb.tiff\nCreating tags for: {'scan_type': 'agb_calibration'}\n{'uid': 'b4d73672-acbf-4d3a-bce3-6b4aedc82664', 'time': 1600040902.9837074, 'scan_type': 'agb_calibration'}\n"
    }
   ],
   "source": [
    "for file_path in paths:\n",
    "    print(file_path)\n",
    "\n",
    "    # extract and transform each file, creating a small jpg and small npy file\n",
    "    raw_metadata, thumb_metadatas, return_metadata = etl_executor.execute(\n",
    "                        file_path, [(223, 'jpg'), (223, 'npy')])\n",
    "    print(\"\\nreading and transforming file: \" + file_path)\n",
    "\n",
    "    # generate documentstream for each \n",
    "    docs = etl.ingest.createDocument(raw_metadata, output_dir,\n",
    "                    thumb_metadatas, return_metadata)\n",
    "\n",
    "    for name, doc in docs:\n",
    "        # serialize the documents (into mongo)\n",
    "        serializer(name, doc)\n",
    "        if name == 'start':\n",
    "            print(\"Creating tags for: \" + repr(return_metadata))\n",
    "\n",
    "            # now let's create tag sets in the TagService\n",
    "            print(doc)\n",
    "            tag_set = make_tag_set(doc)\n",
    "            tag_svc.create_asset_tags(tag_set, tagging_event_uid)\n"
   ]
  },
  {
   "source": [
    "Now that we have loaded the tagging database, we can do some queries on what we have. First, find random tagging events."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[{'sample_id': 'random',\n  'tags': [{'tag': 'agb', 'confidence': 0.99, 'event_id': ''}],\n  'uid': 'bebade35-dd79-4ff7-a00b-0d738945e919',\n  'schema_version': '0.01',\n  '_id': ObjectId('5f5eafc6d1a5ea91881fda46')}]"
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "source": [
    "list(tag_svc.find_random_asset_sets(1))"
   ]
  },
  {
   "source": [
    "We query based on tags. (note that this signature will be enhanced to make confidence parameters a range and optional)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[{'sample_id': 'random',\n  'tags': [{'tag': 'sample', 'confidence': 0.99, 'event_id': ''}],\n  'uid': 'f7e732de-6f91-451d-85fd-11983665a879',\n  'schema_version': '0.01',\n  '_id': ObjectId('5f5eafc6d1a5ea91881fda39')}]"
     },
     "metadata": {},
     "execution_count": 122
    }
   ],
   "source": [
    "list(tag_svc.find_tags_one_filter('sample', 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}